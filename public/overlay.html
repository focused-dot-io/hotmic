<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <title>Recording</title>
  <style>
    body {
      margin: 0;
      padding: 0;
      background-color: transparent;
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, sans-serif;
      overflow: hidden;
      user-select: none;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
    }

    .overlay-container {
      width: 200px;
      height: 200px;
      border-radius: 50%;
      background-color: rgba(0, 0, 0, 0.4);
      backdrop-filter: blur(10px);
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      color: white;
      text-align: center;
      padding: 10px;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.3);
      position: relative;
      overflow: hidden;
    }

    .recording-indicator {
      width: 20px;
      height: 20px;
      border-radius: 50%;
      background-color: #ff3b30;
      margin-bottom: 10px;
      animation: pulse 1.5s infinite;
    }

    .audio-level {
      width: 160px;
      height: 6px;
      background-color: rgba(255, 255, 255, 0.2);
      border-radius: 3px;
      margin: 10px 0;
      overflow: hidden;
    }

    .audio-level-fill {
      height: 100%;
      width: 0%;
      background-color: #34c759;
      border-radius: 3px;
      transition: width 0.1s ease-out;
    }

    .status-message {
      font-size: 14px;
      margin: 5px 0;
      height: 20px;
    }

    .time-display {
      font-size: 24px;
      font-weight: bold;
      margin: 10px 0;
    }

    .progress-bar {
      width: 160px;
      height: 6px;
      background-color: rgba(255, 255, 255, 0.2);
      border-radius: 3px;
      margin: 10px 0;
      display: none;
    }

    .progress-bar-fill {
      height: 100%;
      width: 0%;
      background-color: #007aff;
      border-radius: 3px;
      transition: width 0.3s ease;
    }

    @keyframes pulse {
      0% {
        opacity: 1;
        transform: scale(1);
      }

      50% {
        opacity: 0.5;
        transform: scale(1.1);
      }

      100% {
        opacity: 1;
        transform: scale(1);
      }
    }

    .transcription-icon {
      font-size: 24px;
      margin-bottom: 10px;
      display: none;
    }

    .fade-out {
      animation: fadeOut 0.5s forwards;
    }

    @keyframes fadeOut {
      from {
        opacity: 1;
      }

      to {
        opacity: 0;
      }
    }

    .spinner {
      display: none;
      width: 30px;
      height: 30px;
      border: 3px solid rgba(255, 255, 255, 0.3);
      border-radius: 50%;
      border-top-color: white;
      animation: spin 1s linear infinite;
      margin-bottom: 10px;
    }

    @keyframes spin {
      0% {
        transform: rotate(0deg);
      }

      100% {
        transform: rotate(360deg);
      }
    }
  </style>
</head>

<body>
  <div class="overlay-container">
    <div class="recording-indicator"></div>
    <div class="spinner"></div>
    <div class="transcription-icon">üìù</div>
    <div class="time-display">00:00</div>
    <div class="audio-level">
      <div class="audio-level-fill"></div>
    </div>
    <div class="status-message">Recording...</div>
    <div class="progress-bar">
      <div class="progress-bar-fill"></div>
    </div>
  </div>

  <script>
    /**
     * Whisper Transcriber Overlay
     *
     * Handles audio recording and displays visual feedback
     * for recording and transcription processes.
     */

    // DOM elements
    const elements = {
      recordingIndicator: document.querySelector('.recording-indicator'),
      audioLevelFill: document.querySelector('.audio-level-fill'),
      statusMessage: document.querySelector('.status-message'),
      timeDisplay: document.querySelector('.time-display'),
      progressBar: document.querySelector('.progress-bar'),
      progressBarFill: document.querySelector('.progress-bar-fill'),
      spinner: document.querySelector('.spinner'),
      transcriptionIcon: document.querySelector('.transcription-icon')
    };

    // Application state
    const state = {
      isRecording: false,
      recordingStartTime: null,
      recordingInterval: null,
      audioContext: null,
      analyser: null,
      dataArray: null,
      mediaRecorder: null,
      audioData: []
    };

    /**
     * Recording functionality
     */

    // Update recording time display
    function updateTimeDisplay() {
      if (!state.recordingStartTime) return;

      const elapsed = Date.now() - state.recordingStartTime;
      const seconds = Math.floor((elapsed / 1000) % 60);
      const minutes = Math.floor((elapsed / 1000 / 60) % 60);

      elements.timeDisplay.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
    }

    // Calculate audio level from analyzer data
    function calculateAudioLevel(array) {
      let sum = 0;
      for (let i = 0; i < array.length; i++) {
        sum += array[i] / 255;
      }
      return sum / array.length;
    }

    // Monitor and display audio levels
    function monitorAudioLevel() {
      if (!state.analyser) return;

      state.analyser.getByteFrequencyData(state.dataArray);
      const level = calculateAudioLevel(state.dataArray);

      // Update UI
      elements.audioLevelFill.style.width = `${level * 100}%`;

      // Send to main process
      window.api.sendAudioLevel(level);

      // Continue monitoring
      requestAnimationFrame(monitorAudioLevel);
    }

    // Start audio recording
    async function startRecording() {
      try {
        // Reset state
        state.isRecording = true;
        state.audioData = [];
        state.recordingStartTime = Date.now();

        // Update UI for recording state
        setRecordingUI();

        // Start timer
        state.recordingInterval = setInterval(updateTimeDisplay, 1000);
        updateTimeDisplay();

        // Get audio stream
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // Setup audio context for level monitoring
        setupAudioAnalyzer(stream);

        // Setup media recorder
        setupMediaRecorder(stream);

        // Start recording
        state.mediaRecorder.start();
      } catch (error) {
        console.error('Recording error:', error);
        elements.statusMessage.textContent = `Error: ${error.message}`;
      }
    }

    // Set up audio analyzer for level monitoring
    function setupAudioAnalyzer(stream) {
      state.audioContext = new AudioContext();
      const source = state.audioContext.createMediaStreamSource(stream);
      state.analyser = state.audioContext.createAnalyser();
      state.analyser.fftSize = 256;
      source.connect(state.analyser);
      state.dataArray = new Uint8Array(state.analyser.frequencyBinCount);

      // Start monitoring audio levels
      monitorAudioLevel();
    }

    // Set up media recorder
    function setupMediaRecorder(stream) {
      state.mediaRecorder = new MediaRecorder(stream);

      state.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          state.audioData.push(event.data);
        }
      };

      state.mediaRecorder.onstop = async () => {
        // Convert to blob then to array buffer
        const blob = new Blob(state.audioData, { type: 'audio/wav' });
        const arrayBuffer = await blob.arrayBuffer();

        // Clean up
        stream.getTracks().forEach(track => track.stop());

        // Show processing UI
        setProcessingUI();

        // Send to main process
        window.api.sendAudioData(arrayBuffer);
      };
    }

    // Stop recording
    function stopRecording() {
      if (!state.isRecording || !state.mediaRecorder) return;

      state.isRecording = false;

      // Stop timer
      clearInterval(state.recordingInterval);

      // Stop media recorder
      if (state.mediaRecorder && state.mediaRecorder.state !== 'inactive') {
        state.mediaRecorder.stop();
      }

      // Stop audio context
      if (state.audioContext) {
        state.audioContext.close();
        state.audioContext = null;
        state.analyser = null;
      }
    }

    /**
     * UI State Management
     */

    // Set UI for recording state
    function setRecordingUI() {
      elements.recordingIndicator.style.display = 'block';
      elements.transcriptionIcon.style.display = 'none';
      elements.spinner.style.display = 'none';
      elements.progressBar.style.display = 'none';
      elements.audioLevelFill.style.width = '0%';
      elements.statusMessage.textContent = 'Recording...';
    }

    // Set UI for processing state
    function setProcessingUI() {
      elements.recordingIndicator.style.display = 'none';
      elements.spinner.style.display = 'block';
      elements.progressBar.style.display = 'block';
      elements.statusMessage.textContent = 'Processing...';
    }

    // Handle transcription progress updates
    function handleTranscriptionProgress(data) {
      const progressMap = {
        'start': { text: 'Starting...', progress: 10 },
        'api': { text: 'Sending to API...', progress: 30 },
        'receiving': { text: 'Receiving data...', progress: 70 },
        'complete': { text: 'Copied to clipboard!', progress: 100 },
        'error': { text: data.message, progress: 100 }
      };

      const progressInfo = progressMap[data.step] || { text: 'Processing...', progress: 50 };

      // Update UI
      elements.statusMessage.textContent = progressInfo.text;
      elements.progressBarFill.style.width = `${progressInfo.progress}%`;

      // For completion, show checkmark instead of spinner
      if (data.step === 'complete') {
        elements.spinner.style.display = 'none';
        elements.transcriptionIcon.style.display = 'block';
      }

      // For error, change colors
      if (data.step === 'error') {
        elements.progressBarFill.style.backgroundColor = '#ff3b30';
      }
    }

    /**
     * Event Handlers
     */

    // Register event listeners
    function registerEventListeners() {
      // Listen for start recording event
      window.api.onStartRecording(() => {
        startRecording();
      });

      // Listen for stop recording event
      window.api.onStopRecording(() => {
        stopRecording();
      });

      // Listen for audio level events (from other windows)
      window.api.onAudioLevel((level) => {
        elements.audioLevelFill.style.width = `${level * 100}%`;
      });

      // Listen for transcription progress
      window.api.onTranscriptionProgress(handleTranscriptionProgress);
    }

    // Initialize when DOM is loaded
    document.addEventListener('DOMContentLoaded', registerEventListeners);
  </script>
</body>

</html>