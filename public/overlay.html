<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <title>Recording</title>
  <style>
    body {
      margin: 0;
      padding: 0;
      background-color: transparent;
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      overflow: hidden;
      user-select: none;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
    }

    .overlay-container {
      width: 200px;
      height: 200px;
      border-radius: 50%;
      background-color: rgba(0, 0, 0, 0.4);
      backdrop-filter: blur(10px);
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      color: white;
      text-align: center;
      padding: 10px;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.3);
      position: relative;
      overflow: hidden;
    }

    .recording-indicator {
      width: 20px;
      height: 20px;
      border-radius: 50%;
      background-color: #ff3b30;
      margin-bottom: 10px;
      animation: pulse 1.5s infinite;
    }

    .audio-level {
      width: 160px;
      height: 6px;
      background-color: rgba(255, 255, 255, 0.2);
      border-radius: 3px;
      margin: 10px 0;
      overflow: hidden;
    }

    .audio-level-fill {
      height: 100%;
      width: 0%;
      background-color: #34c759;
      border-radius: 3px;
      transition: width 0.1s ease-out;
    }

    .status-message {
      font-size: 14px;
      margin: 5px 0;
      height: 20px;
    }

    .time-display {
      font-size: 24px;
      font-weight: bold;
      margin: 10px 0;
    }

    .progress-bar {
      width: 160px;
      height: 6px;
      background-color: rgba(255, 255, 255, 0.2);
      border-radius: 3px;
      margin: 10px 0;
      display: none;
    }

    .progress-bar-fill {
      height: 100%;
      width: 0%;
      background-color: #007aff;
      border-radius: 3px;
      transition: width 0.3s ease;
    }

    @keyframes pulse {
      0% {
        opacity: 1;
        transform: scale(1);
      }

      50% {
        opacity: 0.5;
        transform: scale(1.1);
      }

      100% {
        opacity: 1;
        transform: scale(1);
      }
    }

    .transcription-icon {
      font-size: 24px;
      margin-bottom: 10px;
      display: none;
    }

    .fade-out {
      animation: fadeOut 0.5s forwards;
    }

    @keyframes fadeOut {
      from {
        opacity: 1;
      }

      to {
        opacity: 0;
      }
    }

    .spinner {
      display: none;
      width: 30px;
      height: 30px;
      border: 3px solid rgba(255, 255, 255, 0.3);
      border-radius: 50%;
      border-top-color: white;
      animation: spin 1s linear infinite;
      margin-bottom: 10px;
    }

    @keyframes spin {
      0% {
        transform: rotate(0deg);
      }

      100% {
        transform: rotate(360deg);
      }
    }
  </style>
</head>

<body>
  <div class="overlay-container">
    <div class="recording-indicator"></div>
    <div class="spinner"></div>
    <div class="transcription-icon">üìù</div>
    <div class="time-display">00:00</div>
    <div class="audio-level">
      <div class="audio-level-fill"></div>
    </div>
    <div class="status-message">Recording...</div>
    <div class="progress-bar">
      <div class="progress-bar-fill"></div>
    </div>
  </div>

  <script>
    // DOM elements
    const recordingIndicator = document.querySelector('.recording-indicator');
    const audioLevelFill = document.querySelector('.audio-level-fill');
    const statusMessage = document.querySelector('.status-message');
    const timeDisplay = document.querySelector('.time-display');
    const progressBar = document.querySelector('.progress-bar');
    const progressBarFill = document.querySelector('.progress-bar-fill');
    const spinner = document.querySelector('.spinner');
    const transcriptionIcon = document.querySelector('.transcription-icon');

    // Recording state
    let isRecording = false;
    let recordingStartTime = null;
    let recordingInterval = null;
    let mediaRecorder = null;
    let audioData = [];

    // Audio context for level monitoring
    let audioContext = null;
    let analyser = null;
    let dataArray = null;

    // Update recording time
    function updateTime() {
      if (!recordingStartTime) return;

      const elapsed = Date.now() - recordingStartTime;
      const seconds = Math.floor((elapsed / 1000) % 60);
      const minutes = Math.floor((elapsed / 1000 / 60) % 60);

      timeDisplay.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
    }

    // Calculate audio level
    function calculateAudioLevel(array) {
      let sum = 0;
      for (let i = 0; i < array.length; i++) {
        sum += array[i] / 255;
      }
      return sum / array.length;
    }

    // Monitor audio levels
    function monitorAudioLevel() {
      if (!analyser) return;

      analyser.getByteFrequencyData(dataArray);
      const level = calculateAudioLevel(dataArray);

      // Update UI
      audioLevelFill.style.width = `${level * 100}%`;

      // Send to main process
      window.api.sendAudioLevel(level);

      // Schedule next update
      requestAnimationFrame(monitorAudioLevel);
    }

    // Start recording
    async function startRecording() {
      try {
        // Reset state
        isRecording = true;
        audioData = [];
        recordingStartTime = Date.now();

        // Update UI
        recordingIndicator.style.display = 'block';
        transcriptionIcon.style.display = 'none';
        spinner.style.display = 'none';
        progressBar.style.display = 'none';
        audioLevelFill.style.width = '0%';
        statusMessage.textContent = 'Recording...';

        // Start timer
        recordingInterval = setInterval(updateTime, 1000);
        updateTime();

        // Get audio stream
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // Setup audio context for level monitoring
        audioContext = new AudioContext();
        const source = audioContext.createMediaStreamSource(stream);
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        source.connect(analyser);
        dataArray = new Uint8Array(analyser.frequencyBinCount);

        // Start monitoring audio levels
        monitorAudioLevel();

        // Setup media recorder
        mediaRecorder = new MediaRecorder(stream);

        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioData.push(event.data);
          }
        };

        mediaRecorder.onstop = async () => {
          // Convert to blob then to array buffer
          const blob = new Blob(audioData, { type: 'audio/wav' });
          const arrayBuffer = await blob.arrayBuffer();

          // Clean up
          stream.getTracks().forEach(track => track.stop());

          // Show processing UI
          recordingIndicator.style.display = 'none';
          spinner.style.display = 'block';
          progressBar.style.display = 'block';
          statusMessage.textContent = 'Processing...';

          // Send to main process
          window.api.sendAudioData(arrayBuffer);
        };

        // Start recording
        mediaRecorder.start();
      } catch (error) {
        console.error('Recording error:', error);
        statusMessage.textContent = `Error: ${error.message}`;
      }
    }

    // Stop recording
    function stopRecording() {
      if (!isRecording || !mediaRecorder) return;

      isRecording = false;

      // Stop timer
      clearInterval(recordingInterval);

      // Stop media recorder
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
      }

      // Stop audio context
      if (audioContext) {
        audioContext.close();
        audioContext = null;
        analyser = null;
      }
    }

    // Handle transcription progress
    function handleTranscriptionProgress(data) {
      switch (data.step) {
        case 'start':
          statusMessage.textContent = 'Starting...';
          progressBarFill.style.width = '10%';
          break;
        case 'api':
          statusMessage.textContent = 'Sending to API...';
          progressBarFill.style.width = '30%';
          break;
        case 'receiving':
          statusMessage.textContent = 'Receiving data...';
          progressBarFill.style.width = '70%';
          break;
        case 'complete':
          statusMessage.textContent = 'Copied to clipboard!';
          progressBarFill.style.width = '100%';
          spinner.style.display = 'none';
          transcriptionIcon.style.display = 'block';
          break;
        case 'error':
          statusMessage.textContent = data.message;
          break;
      }
    }

    // Register event listeners
    window.addEventListener('DOMContentLoaded', () => {
      // Listen for start recording event
      window.api.onStartRecording(() => {
        startRecording();
      });

      // Listen for stop recording event
      window.api.onStopRecording(() => {
        stopRecording();
      });

      // Listen for audio level events (from other windows)
      window.api.onAudioLevel((level) => {
        audioLevelFill.style.width = `${level * 100}%`;
      });

      // Listen for transcription progress
      window.api.onTranscriptionProgress(handleTranscriptionProgress);
    });
  </script>
</body>

</html>